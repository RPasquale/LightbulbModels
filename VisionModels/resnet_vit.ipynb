{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], BBox Loss: 33534.72290080654, Class Loss: 3.450539509726139\n",
      "Epoch [2/10], BBox Loss: 32253.073559944634, Class Loss: 3.385037877592155\n",
      "Epoch [3/10], BBox Loss: 32157.05880503354, Class Loss: 3.4155356373250787\n",
      "Epoch [4/10], BBox Loss: 32182.724289954218, Class Loss: 3.402547035651881\n",
      "Epoch [5/10], BBox Loss: 32175.39709826182, Class Loss: 3.343142723001451\n",
      "Epoch [6/10], BBox Loss: 32211.49145383039, Class Loss: 3.3637689298218723\n",
      "Epoch [7/10], BBox Loss: 32110.958244050787, Class Loss: 3.3236998879117494\n",
      "Epoch [8/10], BBox Loss: 32145.606372111903, Class Loss: 3.267943659165852\n",
      "Epoch [9/10], BBox Loss: 32149.192091673765, Class Loss: 3.3338012232284937\n",
      "Epoch [10/10], BBox Loss: 32190.174590076662, Class Loss: 3.2974150052874824\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from datasets import load_dataset\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "# Custom Dataset Class for COCO Data\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None, max_objects=10):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.max_objects = max_objects\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        record = self.dataset[idx]\n",
    "        image = record['image'].convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        bboxes = record['objects']['bbox']\n",
    "        labels = record['objects']['category']\n",
    "        \n",
    "        # Pad bboxes and labels\n",
    "        bboxes = bboxes + [[0, 0, 0, 0]] * (self.max_objects - len(bboxes))\n",
    "        labels = labels + [0] * (self.max_objects - len(labels))\n",
    "        \n",
    "        return image, torch.tensor(bboxes[:self.max_objects], dtype=torch.float32), torch.tensor(labels[:self.max_objects], dtype=torch.long)\n",
    "\n",
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"detection-datasets/coco\", split='train[:1%]')\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "coco_dataset = COCODataset(dataset, transform=transform)\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    images, bboxes, labels = zip(*batch)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    bboxes = torch.stack(bboxes, dim=0)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "    return images, bboxes, labels\n",
    "\n",
    "coco_dataloader = DataLoader(coco_dataset, batch_size=2, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Load pre-trained ResNet model\n",
    "resnet_model = models.resnet50(pretrained=True)\n",
    "resnet_model.eval()\n",
    "resnet_model = torch.nn.Sequential(*list(resnet_model.children())[:-1])\n",
    "\n",
    "def extract_features(dataloader, model, max_objects):\n",
    "    all_features = []\n",
    "    all_bboxes = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, bboxes, labels in dataloader:\n",
    "            outputs = model(images)\n",
    "            features = outputs.view(outputs.size(0), -1)  # Flatten the tensor\n",
    "            features = features.unsqueeze(1).expand(-1, max_objects, -1)  # (batch_size, max_objects, feature_dim)\n",
    "            all_features.append(features)\n",
    "            all_bboxes.append(bboxes)\n",
    "            all_labels.append(labels)\n",
    "    \n",
    "    return torch.cat(all_features), torch.cat(all_bboxes), torch.cat(all_labels)\n",
    "\n",
    "# Extract features from COCO data\n",
    "features, bboxes, labels = extract_features(coco_dataloader, resnet_model, max_objects=10)\n",
    "\n",
    "class ResNetFeatureDataset(Dataset):\n",
    "    def __init__(self, features, bboxes, labels):\n",
    "        self.features = features\n",
    "        self.bboxes = bboxes\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        bbox = self.bboxes[idx]\n",
    "        label = self.labels[idx]\n",
    "        return feature, bbox, label\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "feature_dataset = ResNetFeatureDataset(features, bboxes, labels)\n",
    "feature_dataloader = DataLoader(feature_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, feature_dim, num_classes, num_heads=8, num_layers=6, max_objects=10):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.max_objects = max_objects\n",
    "        self.pos_encoder = PositionalEncoding(feature_dim)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=feature_dim, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.fc_bbox = nn.Linear(feature_dim, 4)\n",
    "        self.fc_class = nn.Linear(feature_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(f\"Input features shape: {x.shape}\")\n",
    "        x = x.permute(1, 0, 2)  # Change shape to (seq_len, batch_size, feature_dim)\n",
    "        #print(f\"Reshaped and permuted shape: {x.shape}\")\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.permute(1, 0, 2)  # Change shape back to (batch_size, max_objects, feature_dim)\n",
    "        #print(f\"After transformer: {x.shape}\")\n",
    "        bboxes = self.fc_bbox(x)\n",
    "        classes = self.fc_class(x)\n",
    "        return bboxes, classes\n",
    "\n",
    "# Define the model\n",
    "feature_dim = 2048  # This is the dimension of the ResNet features\n",
    "num_classes = len(dataset.features['objects'].feature['category'].names)\n",
    "model = TransformerModel(feature_dim, num_classes)\n",
    "\n",
    "criterion_bbox = nn.MSELoss()\n",
    "criterion_class = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, dataloader, criterion_bbox, criterion_class, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss_bbox = 0.0\n",
    "        running_loss_class = 0.0\n",
    "        for i, (features, bboxes, labels) in enumerate(dataloader):\n",
    "            #print(f\"Features shape in batch: {features.shape}\")\n",
    "            optimizer.zero_grad()\n",
    "            outputs_bboxes, outputs_classes = model(features)\n",
    "            #print(f\"Outputs bboxes shape: {outputs_bboxes.shape}\")\n",
    "            #print(f\"Outputs classes shape: {outputs_classes.shape}\")\n",
    "            \n",
    "            # Flatten and print labels to check values\n",
    "            flattened_labels = labels.view(-1)\n",
    "            #print(f\"Labels before passing to loss: {flattened_labels}\")\n",
    "            \n",
    "            # Check if all label values are within the valid range\n",
    "            if torch.any(flattened_labels >= num_classes):\n",
    "                print(f\"Invalid label found: {flattened_labels[flattened_labels >= num_classes]}\")\n",
    "                continue  # Skip this batch if invalid labels are found\n",
    "            \n",
    "            loss_bbox = criterion_bbox(outputs_bboxes, bboxes)\n",
    "            loss_class = criterion_class(outputs_classes.view(-1, num_classes), flattened_labels)\n",
    "            loss = loss_bbox + loss_class\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss_bbox += loss_bbox.item()\n",
    "            running_loss_class += loss_class.item()\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], BBox Loss: {running_loss_bbox / len(dataloader)}, Class Loss: {running_loss_class / len(dataloader)}\")\n",
    "\n",
    "train_model(model, feature_dataloader, criterion_bbox, criterion_class, optimizer)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"transformer_object_detection.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], BBox Loss: 33160.591650473805, Class Loss: 2.3762212528686915\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 205\u001b[0m\n\u001b[0;32m    202\u001b[0m             running_loss_class \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_class\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], BBox Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss_bbox\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Class Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss_class\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 205\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_bbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;66;03m#torch.save(model.state_dict(), \"transformer_object_detection.pth\")\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 199\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader, criterion_bbox, criterion_class, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m    197\u001b[0m loss_class \u001b[38;5;241m=\u001b[39m criterion_class(outputs_classes\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_classes), flattened_labels)\n\u001b[0;32m    198\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_bbox \u001b[38;5;241m+\u001b[39m loss_class\n\u001b[1;32m--> 199\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    201\u001b[0m running_loss_bbox \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_bbox\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\torch_cuda\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\torch_cuda\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\torch_cuda\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from datasets import load_dataset\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "# Custom Dataset Class for COCO Data\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None, max_objects=10):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.max_objects = max_objects\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        record = self.dataset[idx]\n",
    "        image = record['image'].convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        bboxes = record['objects']['bbox']\n",
    "        labels = record['objects']['category']\n",
    "        \n",
    "        # Pad bboxes and labels\n",
    "        bboxes = bboxes + [[0, 0, 0, 0]] * (self.max_objects - len(bboxes))\n",
    "        labels = labels + [0] * (self.max_objects - len(labels))\n",
    "        \n",
    "        return image, torch.tensor(bboxes[:self.max_objects], dtype=torch.float32), torch.tensor(labels[:self.max_objects], dtype=torch.long)\n",
    "\n",
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"detection-datasets/coco\", split='train[:1%]')\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "coco_dataset = COCODataset(dataset, transform=transform)\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    images, bboxes, labels = zip(*batch)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    bboxes = torch.stack(bboxes, dim=0)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "    return images, bboxes, labels\n",
    "\n",
    "coco_dataloader = DataLoader(coco_dataset, batch_size=2, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Load pre-trained ResNet model\n",
    "resnet_model = models.resnet50(pretrained=True)\n",
    "resnet_model.eval()\n",
    "resnet_model = torch.nn.Sequential(*list(resnet_model.children())[:-1])\n",
    "\n",
    "def extract_features(dataloader, model, max_objects):\n",
    "    all_features = []\n",
    "    all_bboxes = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, bboxes, labels in dataloader:\n",
    "            outputs = model(images)\n",
    "            features = outputs.view(outputs.size(0), -1)  # Flatten the tensor\n",
    "            features = features.unsqueeze(1).expand(-1, max_objects, -1)  # (batch_size, max_objects, feature_dim)\n",
    "            all_features.append(features)\n",
    "            all_bboxes.append(bboxes)\n",
    "            all_labels.append(labels)\n",
    "    \n",
    "    return torch.cat(all_features), torch.cat(all_bboxes), torch.cat(all_labels)\n",
    "\n",
    "# Extract features from COCO data\n",
    "features, bboxes, labels = extract_features(coco_dataloader, resnet_model, max_objects=10)\n",
    "\n",
    "class ResNetFeatureDataset(Dataset):\n",
    "    def __init__(self, features, bboxes, labels):\n",
    "        self.features = features\n",
    "        self.bboxes = bboxes\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        bbox = self.bboxes[idx]\n",
    "        label = self.labels[idx]\n",
    "        return feature, bbox, label\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "feature_dataset = ResNetFeatureDataset(features, bboxes, labels)\n",
    "feature_dataloader = DataLoader(feature_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, feature_dim, num_classes, num_heads=8, num_layers=6, max_objects=10):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.max_objects = max_objects\n",
    "        self.pos_encoder = PositionalEncoding(feature_dim)\n",
    "        self.pos_decoder = PositionalEncoding(feature_dim)\n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=feature_dim, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        \n",
    "        decoder_layers = nn.TransformerDecoderLayer(d_model=feature_dim, nhead=num_heads)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layers, num_layers)\n",
    "        \n",
    "        hidden_dim = 512  # You can adjust this value as needed\n",
    "        self.mlp_bbox = MLPBlock(feature_dim, hidden_dim, 4)\n",
    "        self.mlp_class = MLPBlock(feature_dim, hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # Encoder\n",
    "        src = src.permute(1, 0, 2)  # Change shape to (seq_len, batch_size, feature_dim)\n",
    "        src = self.pos_encoder(src)\n",
    "        memory = self.transformer_encoder(src)\n",
    "        \n",
    "        # Decoder\n",
    "        tgt = tgt.permute(1, 0, 2)  # Change shape to (seq_len, batch_size, feature_dim)\n",
    "        tgt = self.pos_decoder(tgt)\n",
    "        output = self.transformer_decoder(tgt, memory)\n",
    "        output = output.permute(1, 0, 2)  # Change shape back to (batch_size, max_objects, feature_dim)\n",
    "        \n",
    "        # Apply MLP blocks\n",
    "        bboxes = self.mlp_bbox(output)\n",
    "        classes = self.mlp_class(output)\n",
    "        \n",
    "        return bboxes, classes\n",
    "\n",
    "# Define the model\n",
    "feature_dim = 2048  # This is the dimension of the ResNet features\n",
    "num_classes = len(dataset.features['objects'].feature['category'].names)\n",
    "model = TransformerModel(feature_dim, num_classes)\n",
    "\n",
    "criterion_bbox = nn.MSELoss()\n",
    "criterion_class = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, dataloader, criterion_bbox, criterion_class, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss_bbox = 0.0\n",
    "        running_loss_class = 0.0\n",
    "        for i, (features, bboxes, labels) in enumerate(dataloader):\n",
    "            # Prepare target input for the decoder (could be shifted version of features)\n",
    "            tgt = features\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs_bboxes, outputs_classes = model(features, tgt)\n",
    "            \n",
    "            # Flatten and print labels to check values\n",
    "            flattened_labels = labels.view(-1)\n",
    "            \n",
    "            # Check if all label values are within the valid range\n",
    "            if torch.any(flattened_labels >= num_classes):\n",
    "                print(f\"Invalid label found: {flattened_labels[flattened_labels >= num_classes]}\")\n",
    "                continue  # Skip this batch if invalid labels are found\n",
    "            \n",
    "            loss_bbox = criterion_bbox(outputs_bboxes, bboxes)\n",
    "            loss_class = criterion_class(outputs_classes.view(-1, num_classes), flattened_labels)\n",
    "            loss = loss_bbox + loss_class\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss_bbox += loss_bbox.item()\n",
    "            running_loss_class += loss_class.item()\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], BBox Loss: {running_loss_bbox / len(dataloader)}, Class Loss: {running_loss_class / len(dataloader)}\")\n",
    "\n",
    "train_model(model, feature_dataloader, criterion_bbox, criterion_class, optimizer)\n",
    "\n",
    "# Save the trained model\n",
    "#torch.save(model.state_dict(), \"transformer_object_detection.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
