{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from transformers import DetrFeatureExtractor, DetrForObjectDetection, ViTFeatureExtractor, ViTModel, DetrImageProcessor\n",
    "from einops import rearrange\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import math\n",
    "from torchvision.transforms import functional as TF\n",
    "from torchvision.transforms import ToTensor, Normalize\n",
    "from collections import Counter\n",
    "\n",
    "class VisionExpert(nn.Module):\n",
    "    def __init__(self, num_classes=80, num_experts=4, pe_type='regular', device='cuda'):\n",
    "        super(VisionExpert, self).__init__()\n",
    "        self.device = device\n",
    "        # Load pretrained models\n",
    "        self.resnet_model = models.resnet152(pretrained=True)\n",
    "\n",
    "        # Remove the last layer of resnet\n",
    "        self.resnet_model = nn.Sequential(*list(self.resnet_model.children())[:-1])\n",
    "        # Positional Encoding\n",
    "        self.pe_type = pe_type\n",
    "        if pe_type == 'regular':\n",
    "            self.pos_encoder = VisionExpert.PositionalEncoding(d_model=2048)\n",
    "        elif pe_type == 'rotary':\n",
    "            self.pos_encoder = VisionExpert.RotaryPositionalEncoding(dim=2048)\n",
    "        elif pe_type == 'contextual':\n",
    "            self.pos_encoder = VisionExpert.ContextualPositionalEncoding(d_model=2048)\n",
    "\n",
    "        # Custom layers\n",
    "        self.layernorm = nn.LayerNorm(normalized_shape=2048)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=2048, num_heads=8)\n",
    "        \n",
    "        # Initialize sub-models\n",
    "        self.vision_transformer = VisionExpert.VisionTransformer(num_classes=num_classes)\n",
    "        self.detection_transformer = VisionExpert.DetectionTransformer(num_classes=num_classes)\n",
    "        self.vision_mamba = VisionExpert.VisionMamba(num_classes=num_classes)\n",
    "        \n",
    "        # Sub Model Router\n",
    "        self.sub_model_gate = nn.Linear(2048, 3)  # 3 sub-models\n",
    "\n",
    "        # Initialize Lory MoE\n",
    "        self.moe_layer = VisionExpert.LORY_MOE(input_dim=768, hidden_dim=512, num_experts=4, num_classes=num_classes, num_layers=3, segment_size=256)\n",
    "        \n",
    "        # Beam Search (using top-k sampling for simplicity)\n",
    "        self.k = 5\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN Image Encoder\n",
    "        cnn_features = self.resnet_model(x).squeeze()\n",
    "        \n",
    "        # Reshape cnn_features to (batch_size, sequence_length, embedding_dim)\n",
    "        cnn_features = cnn_features.view(cnn_features.size(0), -1, 2048).permute(1, 0, 2)\n",
    "\n",
    "        # Positional Encoding\n",
    "        if self.pe_type == 'regular':\n",
    "            cnn_features = self.pos_encoder(cnn_features)\n",
    "        elif self.pe_type == 'rotary':\n",
    "            cnn_features = self.pos_encoder(cnn_features)\n",
    "        elif self.pe_type == 'contextual':\n",
    "            cnn_features = self.pos_encoder(cnn_features, mask=None)  # Update mask if needed\n",
    "\n",
    "        # Blast Attention\n",
    "        attn_output, _ = self.attention(cnn_features, cnn_features, cnn_features)\n",
    "        attn_output = attn_output.permute(1, 0, 2)  # Permute back to (batch_size, sequence_length, embedding_dim)\n",
    "        \n",
    "        # LayerNorm and Dropout\n",
    "        norm_output = self.layernorm(attn_output)\n",
    "        norm_output = self.dropout(norm_output)\n",
    "        \n",
    "        # Sub Model Router\n",
    "        gate_output = self.sub_model_gate(norm_output.mean(dim=1))\n",
    "        sub_model_idx = gate_output.argmax(dim=-1)\n",
    "\n",
    "        class_logits_list = []\n",
    "        bbox_preds_list = []\n",
    "\n",
    "        for i in range(sub_model_idx.size(0)):\n",
    "            if sub_model_idx[i] == 0:\n",
    "                # Use Vision Transformer\n",
    "                class_logits, bbox_preds = self.vision_transformer(x[i].unsqueeze(0))\n",
    "            elif sub_model_idx[i] == 1:\n",
    "                # Use Detection Transformer\n",
    "                image_input = x[i].unsqueeze(0)\n",
    "                # Rescale image values to [0, 1]\n",
    "                mean = torch.tensor([0.485, 0.456, 0.406], device=image_input.device).view(1, 3, 1, 1)\n",
    "                std = torch.tensor([0.229, 0.224, 0.225], device=image_input.device).view(1, 3, 1, 1)\n",
    "                image_input = image_input * std + mean\n",
    "                class_logits, bbox_preds = self.detection_transformer(image_input)\n",
    "            elif sub_model_idx[i] == 2:\n",
    "                # Use Vision Mamba\n",
    "                class_logits, bbox_preds = self.vision_mamba(x[i].unsqueeze(0))\n",
    "            \n",
    "            class_logits_list.append(class_logits)\n",
    "            bbox_preds_list.append(bbox_preds)\n",
    "\n",
    "        class_logits = torch.cat(class_logits_list, dim=0)\n",
    "        bbox_preds = torch.cat(bbox_preds_list, dim=0)\n",
    "\n",
    "        # Ensure the class_logits has 3 dimensions for the MoE layer\n",
    "        class_logits = class_logits.unsqueeze(1)\n",
    "\n",
    "        # MoE Layers\n",
    "        moe_output = self.moe_layer(class_logits)\n",
    "\n",
    "        # Blast Attention\n",
    "        final_output, _ = self.attention(moe_output, moe_output, moe_output)\n",
    "        \n",
    "        # LayerNorm and Dropout\n",
    "        final_output = self.layernorm(final_output)\n",
    "        final_output = self.dropout(final_output)\n",
    "        \n",
    "        # Beam Search Decoding (simplified as top-k sampling)\n",
    "        top_k_output = final_output.topk(self.k, dim=-1).values\n",
    "        \n",
    "        return top_k_output, bbox_preds\n",
    "\n",
    "\n",
    "\n",
    "    def train_model(self, dataloader, num_epochs, learning_rate, mode='full'):\n",
    "        \"\"\"\n",
    "        Train the Vision Expert model.\n",
    "        \n",
    "        Parameters:\n",
    "        - dataloader: DataLoader for training data\n",
    "        - num_epochs: number of epochs to train\n",
    "        - learning_rate: learning rate for optimizer\n",
    "        - mode: 'vit', 'mamba', 'detr', or 'full'\n",
    "        \"\"\"\n",
    "        # Define the optimizer and loss function\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        criterion_cls = nn.CrossEntropyLoss()  # Assuming classification task\n",
    "        criterion_bbox = nn.MSELoss()  # Assuming regression task for bounding boxes\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            for batch in dataloader:\n",
    "                inputs = batch['inputs'].to(self.device)\n",
    "                labels_cls = batch['labels'].to(self.device)\n",
    "                labels_bbox = batch['boxes'].to(self.device) if 'boxes' in batch else None\n",
    "                \n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                if mode == 'vit':\n",
    "                    class_logits, bbox_preds = self.vision_transformer(inputs)\n",
    "                elif mode == 'mamba':\n",
    "                    class_logits, bbox_preds = self.vision_mamba(inputs)\n",
    "                elif mode == 'detr':\n",
    "                    class_logits, bbox_preds = self.detection_transformer(inputs)\n",
    "                elif mode == 'full':\n",
    "                    class_logits, bbox_preds = self(inputs)\n",
    "                else:\n",
    "                    raise ValueError(\"Mode must be 'vit', 'mamba', 'detr', or 'full'\")\n",
    "\n",
    "                # Reshape class_logits to match the shape of labels_cls\n",
    "                batch_size, num_boxes, num_classes = class_logits.size()\n",
    "                class_logits = class_logits.view(-1, num_classes)\n",
    "                labels_cls = labels_cls.view(-1)\n",
    "                \n",
    "                # Filter out -1 labels (used for padding)\n",
    "                valid_idx = labels_cls != -1\n",
    "                class_logits = class_logits[valid_idx]\n",
    "                labels_cls = labels_cls[valid_idx]\n",
    "\n",
    "                loss_cls = criterion_cls(class_logits, labels_cls)\n",
    "                loss_bbox = criterion_bbox(bbox_preds.view(-1, 4), labels_bbox.view(-1, 4)) if labels_bbox is not None else 0\n",
    "                loss = loss_cls + loss_bbox\n",
    "\n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update running loss\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        print(\"Training complete\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ###################################################\n",
    "    # POSITIONAL ENCODING\n",
    "\n",
    "    class PositionalEncoding(nn.Module):\n",
    "        def __init__(self, d_model, max_len=5000):\n",
    "            super(VisionExpert.PositionalEncoding, self).__init__()\n",
    "            pe = torch.zeros(max_len, d_model)\n",
    "            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "            pe[:, 0::2] = torch.sin(position * div_term)\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "            pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "            self.register_buffer('pe', pe)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x + self.pe[:x.size(0), :]\n",
    "            return x\n",
    "        \n",
    "    class RotaryPositionalEncoding(nn.Module):\n",
    "        def __init__(self, dim, max_len=5000):\n",
    "            super(VisionExpert.RotaryPositionalEncoding, self).__init__()\n",
    "            inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "            pos = torch.arange(max_len).float()\n",
    "            sinusoid_inp = torch.einsum('i,j->ij', pos, inv_freq)\n",
    "            emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n",
    "            self.register_buffer('emb', emb)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.apply_rotary_pos_emb(x, self.emb[:x.size(-2), :])\n",
    "\n",
    "        def apply_rotary_pos_emb(self, x, rope):\n",
    "            x = torch.einsum('bnd,d->bnd', x, rope)\n",
    "            return x\n",
    "        \n",
    "    class ContextualPositionalEncoding(nn.Module):\n",
    "        def __init__(self, d_model, max_len=5000):\n",
    "            super(VisionExpert.ContextualPositionalEncoding, self).__init__()\n",
    "            self.d_model = d_model\n",
    "            self.max_len = max_len\n",
    "            self.pe = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
    "\n",
    "        def forward(self, x, mask):\n",
    "            seq_len = x.size(1)\n",
    "            pe = self.pe[:, :seq_len, :]\n",
    "            x = x + pe\n",
    "            return x\n",
    "\n",
    "\n",
    "    ####################################################\n",
    "    # VISION TRANSFORMER\n",
    "    class VisionTransformer(nn.Module):\n",
    "        def __init__(self, num_classes=80):\n",
    "            super(VisionExpert.VisionTransformer, self).__init__()\n",
    "\n",
    "            # Load pretrained ViT model and feature extractor\n",
    "            self.vit_feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "            self.vit_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "            # Define additional layers\n",
    "            self.embedding_layer = nn.Linear(768, 512)\n",
    "            self.layernorm1 = nn.LayerNorm(512)\n",
    "            self.dropout1 = nn.Dropout(p=0.1)\n",
    "            self.attention = nn.MultiheadAttention(embed_dim=512, num_heads=8)\n",
    "            self.layernorm2 = nn.LayerNorm(512)\n",
    "            self.dropout2 = nn.Dropout(p=0.1)\n",
    "            self.moe_layer = nn.Linear(512, 512)\n",
    "            self.classifier = nn.Linear(512, num_classes)\n",
    "            self.bbox_predictor = nn.Linear(512, 4)  # Predicts bounding boxes\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Denormalize the image from range [-2.1179, 2.6400] to [0, 1]\n",
    "            mean = torch.tensor([0.485, 0.456, 0.406], device=x.device).view(1, 3, 1, 1)\n",
    "            std = torch.tensor([0.229, 0.224, 0.225], device=x.device).view(1, 3, 1, 1)\n",
    "            x = x * std + mean\n",
    "\n",
    "            # Extract features using ViT feature extractor\n",
    "            inputs = self.vit_feature_extractor(images=x, return_tensors=\"pt\").to(x.device)\n",
    "            outputs = self.vit_model(**inputs)\n",
    "\n",
    "            # Get the embedding space from ViT model\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]  # Using the [CLS] token output\n",
    "\n",
    "            # Additional layers for downstream tasks\n",
    "            x = self.embedding_layer(embeddings)\n",
    "            x = self.layernorm1(x)\n",
    "            x = self.dropout1(x)\n",
    "\n",
    "            # Blast Attention\n",
    "            x = x.unsqueeze(0)  # MultiheadAttention expects input shape (L, N, E)\n",
    "            attn_output, _ = self.attention(x, x, x)\n",
    "            x = attn_output.squeeze(0)\n",
    "\n",
    "            x = self.layernorm2(x)\n",
    "            x = self.dropout2(x)\n",
    "\n",
    "            # Mixture of Experts layer\n",
    "            x = F.relu(self.moe_layer(x))\n",
    "\n",
    "            # Final classification and bounding box prediction layers\n",
    "            class_logits = self.classifier(x)\n",
    "            bbox_preds = self.bbox_predictor(x)\n",
    "\n",
    "            return class_logits, bbox_preds\n",
    "\n",
    "\n",
    "    ####################################################\n",
    "    # DETECTION TRANSFORMER\n",
    "\n",
    "    class DetectionTransformer(nn.Module):\n",
    "        def __init__(self, num_classes=80):\n",
    "            super(VisionExpert.DetectionTransformer, self).__init__()\n",
    "\n",
    "            # Load pretrained DETR model and feature extractor\n",
    "            self.detr_feature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-50')\n",
    "            self.detr_model = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50')\n",
    "\n",
    "            # Define additional layers\n",
    "            self.embedding_layer = nn.Linear(256, 512)\n",
    "            self.layernorm1 = nn.LayerNorm(512)\n",
    "            self.dropout1 = nn.Dropout(p=0.1)\n",
    "            self.attention = nn.MultiheadAttention(embed_dim=512, num_heads=8)\n",
    "            self.layernorm2 = nn.LayerNorm(512)\n",
    "            self.dropout2 = nn.Dropout(p=0.1)\n",
    "            self.moe_layer = nn.Linear(512, 512)\n",
    "            self.classifier = nn.Linear(512, num_classes)\n",
    "            self.bbox_predictor = nn.Linear(512, 4)  # Predicts bounding boxes\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Extract features using DETR feature extractor\n",
    "            inputs = self.detr_feature_extractor(images=x, return_tensors=\"pt\").to(x.device)\n",
    "            outputs = self.detr_model(**inputs)\n",
    "\n",
    "            # Get the embedding space from DETR model\n",
    "            embeddings = outputs.last_hidden_state  # Using the output embeddings\n",
    "\n",
    "            # Additional layers for downstream tasks\n",
    "            x = self.embedding_layer(embeddings)\n",
    "            x = self.layernorm1(x)\n",
    "            x = self.dropout1(x)\n",
    "\n",
    "            # Blast Attention\n",
    "            x = x.permute(1, 0, 2)  # MultiheadAttention expects input shape (L, N, E)\n",
    "            attn_output, _ = self.attention(x, x, x)\n",
    "            x = attn_output.permute(1, 0, 2)\n",
    "\n",
    "            x = self.layernorm2(x)\n",
    "            x = self.dropout2(x)\n",
    "\n",
    "            # Mixture of Experts layer\n",
    "            x = F.relu(self.moe_layer(x))\n",
    "\n",
    "            # Final classification and bounding box prediction layers\n",
    "            class_logits = self.classifier(x)\n",
    "            bbox_preds = self.bbox_predictor(x)\n",
    "\n",
    "            return class_logits, bbox_preds\n",
    "\n",
    "\n",
    "    ####################################################\n",
    "    # VISION MAMBA\n",
    "\n",
    "    class SSM(nn.Module):\n",
    "        def __init__(self, input_dim, state_dim, hidden_dim):\n",
    "            super(VisionExpert.SSM, self).__init__()\n",
    "            self.state_dim = state_dim\n",
    "            self.A = nn.Parameter(torch.randn(input_dim, state_dim))\n",
    "            self.B = nn.Parameter(torch.randn(state_dim, hidden_dim))\n",
    "            self.C = nn.Parameter(torch.randn(hidden_dim, input_dim))\n",
    "            self.delta = nn.Parameter(torch.randn(state_dim, hidden_dim))\n",
    "            self.activation = nn.SiLU()\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = self.activation(F.conv1d(x, self.A))\n",
    "            B_tilde = torch.einsum('bmn,nk->bmk', self.delta, self.B)\n",
    "            C_tilde = torch.einsum('bmn,nk->bmk', self.delta, self.C)\n",
    "            y = torch.einsum('bmn,bmk->bkn', x, B_tilde)\n",
    "            y = torch.einsum('bkn,bnm->bkm', y, C_tilde)\n",
    "            return y\n",
    "\n",
    "    class VimBlock(nn.Module):\n",
    "        def __init__(self, input_dim, state_dim, hidden_dim):\n",
    "            super(VisionExpert.VimBlock, self).__init__()\n",
    "            self.layernorm1 = nn.LayerNorm(input_dim)\n",
    "            self.layernorm2 = nn.LayerNorm(hidden_dim)\n",
    "            self.ssm_forward = VisionExpert.SSM(input_dim, state_dim, hidden_dim)\n",
    "            self.ssm_backward = VisionExpert.SSM(input_dim, state_dim, hidden_dim)\n",
    "            self.proj = nn.Linear(hidden_dim, input_dim)\n",
    "            self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x_norm = self.layernorm1(x)\n",
    "            x_forward = self.ssm_forward(x_norm)\n",
    "            x_backward = self.ssm_backward(torch.flip(x_norm, dims=[1]))\n",
    "            x_combined = x_forward + torch.flip(x_backward, dims=[1])\n",
    "            x_combined = self.layernorm2(x_combined)\n",
    "            x_combined = self.proj(x_combined)\n",
    "            x_combined = self.dropout(x_combined)\n",
    "            return x + x_combined\n",
    "\n",
    "    class VisionMamba(nn.Module):\n",
    "        def __init__(self, image_size=224, patch_size=16, input_dim=768, state_dim=128, hidden_dim=256, num_classes=80, num_blocks=12):\n",
    "            super(VisionExpert.VisionMamba, self).__init__()\n",
    "            self.patch_size = patch_size\n",
    "            self.num_patches = (image_size // patch_size) ** 2\n",
    "            self.linear_proj = nn.Linear(patch_size * patch_size * 3, input_dim)\n",
    "            self.position_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, input_dim))\n",
    "            self.class_token = nn.Parameter(torch.randn(1, 1, input_dim))\n",
    "            self.vim_blocks = nn.ModuleList([VisionExpert.VimBlock(input_dim, state_dim, hidden_dim) for _ in range(num_blocks)])\n",
    "            self.norm = nn.LayerNorm(input_dim)\n",
    "            self.mlp_head = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, num_classes)\n",
    "            )\n",
    "            self.bbox_predictor = nn.Linear(hidden_dim, 4)  # Predicts bounding boxes\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Preprocess input images into patches\n",
    "            B, C, H, W = x.shape\n",
    "            x = rearrange(x, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=self.patch_size, p2=self.patch_size)\n",
    "            x = self.linear_proj(x)\n",
    "            \n",
    "            # Add class token and position embeddings\n",
    "            class_tokens = self.class_token.expand(B, -1, -1)\n",
    "            x = torch.cat((class_tokens, x), dim=1)\n",
    "            x = x + self.position_embedding\n",
    "            \n",
    "            # Pass through Vim blocks\n",
    "            for block in self.vim_blocks:\n",
    "                x = block(x)\n",
    "            \n",
    "            # Classification head\n",
    "            x = self.norm(x)\n",
    "            class_token_final = x[:, 0]\n",
    "            logits = self.mlp_head(class_token_final)\n",
    "            bbox_preds = self.bbox_predictor(x.mean(dim=1))\n",
    "\n",
    "            return logits, bbox_preds\n",
    "\n",
    "    ####################################################\n",
    "    # LORY MIXTURE OF EXPERTS\n",
    "    class Expert(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim):\n",
    "            super(VisionExpert.Expert, self).__init__()\n",
    "            self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "            self.fc2 = nn.Linear(hidden_dim, input_dim)\n",
    "            self.activation = nn.ReLU()\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.activation(self.fc1(x))\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n",
    "    class Router(nn.Module):\n",
    "        def __init__(self, input_dim, num_experts):\n",
    "            super(VisionExpert.Router, self).__init__()\n",
    "            self.fc = nn.Linear(input_dim, num_experts)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return F.softmax(self.fc(x), dim=-1)\n",
    "\n",
    "    class MoELayer(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim, num_experts):\n",
    "            super(VisionExpert.MoELayer, self).__init__()\n",
    "            self.experts = nn.ModuleList([VisionExpert.Expert(input_dim, hidden_dim) for _ in range(num_experts)])\n",
    "            self.router = VisionExpert.Router(input_dim, num_experts)\n",
    "\n",
    "        def forward(self, x, segment_size):\n",
    "            B, L, D = x.size()\n",
    "            num_segments = L // segment_size\n",
    "            x_segments = x.view(B * num_segments, segment_size, D)\n",
    "            \n",
    "            output_segments = []\n",
    "            for i in range(num_segments):\n",
    "                segment = x_segments[:, i, :]\n",
    "                routing_weights = self.router(segment.mean(dim=1))\n",
    "                merged_expert_params = torch.stack([weight * expert(segment) for weight, expert in zip(routing_weights.T, self.experts)], dim=0).sum(dim=0)\n",
    "                output_segments.append(merged_expert_params)\n",
    "\n",
    "            output = torch.cat(output_segments, dim=1)\n",
    "            return output\n",
    "\n",
    "    class LORY_MOE(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim, num_experts, num_classes, num_layers, segment_size):\n",
    "            super(VisionExpert.LORY_MOE, self).__init__()\n",
    "            self.segment_size = segment_size\n",
    "            self.moelayers = nn.ModuleList([VisionExpert.MoELayer(input_dim, hidden_dim, num_experts) for _ in range(num_layers)])\n",
    "            self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "        def forward(self, x):\n",
    "            for moelayer in self.moelayers:\n",
    "                x = moelayer(x, self.segment_size)\n",
    "            x = x.mean(dim=1)\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "# Train Vision Expert on COCO\n",
    "\n",
    "# Initialize the processor\n",
    "# Load the dataset\n",
    "processor = DetrImageProcessor.from_pretrained('facebook/detr-resnet-50')\n",
    "\n",
    "dataset = load_dataset(\"detection-datasets/coco\", split='train[:1%]')\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = TF.resize(image, (800, 800))\n",
    "    image = ToTensor()(image)  # Converts to [0, 1]\n",
    "\n",
    "    # Ensure that the image values are in the expected range [0, 1] after normalization\n",
    "    print(f\"Min and Max pixel values after normalization: {image.min()}, {image.max()}\")\n",
    "    return image\n",
    "\n",
    "class CustomCocoDataset(Dataset):\n",
    "    def __init__(self, dataset, target_size=(800, 800)):\n",
    "        self.dataset = dataset\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        image = sample['image']\n",
    "\n",
    "        # Ensure image is in RGB\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "        # Preprocess image\n",
    "        image = preprocess_image(image)\n",
    "        #print(f\"Image pixel values range before processor: {image.min()} to {image.max()}\")\n",
    "\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "        # Scale bounding boxes according to the new image size\n",
    "        scale_x = self.target_size[0] / sample['width']\n",
    "        scale_y = self.target_size[1] / sample['height']\n",
    "        bboxes = []\n",
    "        for box in sample['objects']['bbox']:\n",
    "            x0 = box[0] * scale_x\n",
    "            y0 = box[1] * scale_y\n",
    "            x1 = (box[0] + box[2]) * scale_x\n",
    "            y1 = (box[1] + box[3]) * scale_y\n",
    "            bboxes.append([x0, y0, x1, y1])\n",
    "        tensor_boxes = torch.tensor(bboxes, dtype=torch.float32)\n",
    "\n",
    "        labels = torch.tensor(sample['objects']['category'], dtype=torch.long)\n",
    "        #print(f\" inputs : {inputs['pixel_values'].squeeze(0)}, 'boxes': {tensor_boxes}, 'labels': {labels}\")\n",
    "        return {'inputs': inputs['pixel_values'].squeeze(0), 'boxes': tensor_boxes, 'labels': labels}\n",
    "\n",
    "# Adjust collate function as needed:\n",
    "def collate_fn(batch):\n",
    "    inputs = torch.stack([item['inputs'] for item in batch])\n",
    "    max_boxes = max(len(item['boxes']) for item in batch)\n",
    "    padded_boxes = torch.zeros((len(batch), max_boxes, 4))\n",
    "    box_masks = torch.zeros((len(batch), max_boxes), dtype=torch.bool)\n",
    "    padded_labels = torch.full((len(batch), max_boxes), -1)  # Fill labels that are not present with -1\n",
    "\n",
    "    for i, item in enumerate(batch):\n",
    "        num_boxes = item['boxes'].shape[0]\n",
    "        padded_boxes[i, :num_boxes] = item['boxes']\n",
    "        padded_labels[i, :num_boxes] = item['labels']\n",
    "        box_masks[i, :num_boxes] = 1\n",
    "\n",
    "    #print(f\"batch size: {len(batch)}, inputs: {inputs}, boxes: {padded_boxes}, labels: {padded_labels}, box_masks: {box_masks} \")\n",
    "    return {'inputs': inputs, 'boxes': padded_boxes, 'labels': padded_labels, 'box_masks': box_masks}\n",
    "\n",
    "# Create the dataset and data loader\n",
    "coco_dataset = CustomCocoDataset(dataset)\n",
    "dataloader = DataLoader(coco_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "'''# Fetch the first batch\n",
    "for batch in dataloader:\n",
    "    print(\"Batch overview:\")\n",
    "    print(f\"Inputs Shape: {batch['inputs'].shape}\")  # Should be [batch_size, C, H, W]\n",
    "    print(f\"Boxes Shape: {batch['boxes'].shape}\")  # Should be [batch_size, max_boxes_per_image, 4]\n",
    "    print(f\"Labels Shape: {batch['labels'].shape}\")  # Should be [batch_size, max_boxes_per_image]\n",
    "    print(f\"Box Masks Shape: {batch['box_masks'].shape}\")  # Should be [batch_size, max_boxes_per_image]\n",
    "\n",
    "    # Optionally, you can print more detailed information about a single sample in the batch\n",
    "    print(\"\\nDetailed view of first sample in batch:\")\n",
    "    print(f\"First Sample - Pixel Values (inputs): {batch['inputs'][0]}\")  # Showing the actual pixel values can be too verbose, consider showing stats\n",
    "    print(f\"First Sample - Bounding Boxes: {batch['boxes'][0]}\")\n",
    "    print(f\"First Sample - Labels: {batch['labels'][0]}\")\n",
    "    print(f\"First Sample - Box Masks: {batch['box_masks'][0]}\")\n",
    "\n",
    "    # Since we only want to check the first batch, break after the first iteration\n",
    "    break'''\n",
    "\n",
    "\n",
    "# Function to collect labels from the dataset\n",
    "def get_unique_labels(dataset):\n",
    "    label_counter = Counter()\n",
    "    for sample in dataset:\n",
    "        labels = sample['objects']['category']\n",
    "        label_counter.update(labels)\n",
    "    return label_counter\n",
    "\n",
    "'''# Collecting unique labels\n",
    "unique_labels = get_unique_labels(dataset)\n",
    "print(\"Unique labels and their counts:\")\n",
    "for label, count in unique_labels.items():\n",
    "    print(f\"Label {label}: {count} occurrences\")'''\n",
    "\n",
    "# Usage example\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VisionExpert().to(device)\n",
    "\n",
    "'''# Train with different modes\n",
    "print(\"Training Vision Transformer...\")\n",
    "model.train_model(dataloader, num_epochs=1, learning_rate=1e-4, mode='vit')\n",
    "\n",
    "print(\"Training Vision Mamba...\")\n",
    "model.train_model(dataloader, num_epochs=1, learning_rate=1e-4, mode='mamba')\n",
    "\n",
    "print(\"Training Detection Transformer...\")\n",
    "model.train_model(dataloader, num_epochs=1, learning_rate=1e-4, mode='detr')\n",
    "'''\n",
    "print(\"Training Full Vision Expert...\")\n",
    "model.train_model(dataloader, num_epochs=1, learning_rate=1e-4, mode='full')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
